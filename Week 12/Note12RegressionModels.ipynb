{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Regression Models\n",
    "\n",
    "Machine Learning에서 Regression은 주된 분야가 아니다. Model 자체가 분류를 목적으로 만들어졌기 때문이다. 그 중 대표적인 Regression Model에 대한 개념들을 살펴보는 것으로 한다"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c11a21961d77a03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Bias - Variance Dilemma\n",
    "\n",
    "다음 회귀모형을 고려해 보자\n",
    "\n",
    "$$y_t = \\beta_0 + \\beta_1 x_t + \\epsilon_t, ~~~~~ \\epsilon_t \\sim N(0,\\sigma_\\epsilon^2) $$\n",
    "\n",
    "오차항은 다음과 같이 다시 쓸 수 있다\n",
    "\n",
    "$$\\epsilon_t = y_t - \\hat{\\beta_0} - \\hat \\beta_1 x_t$$\n",
    "\n",
    "이를 제곱한 값을 Mean Squared Error라고 하는데, MSE는 다음과 같이 분해할 수 있다\n",
    "\n",
    "$$\\mathrm{MSE} = (E[\\beta_0 + \\beta_1 x_t - \\hat \\beta_0 - \\hat \\beta_1 x_t])^2 + \\mathrm{Var}(\\hat\\beta_0 + \\hat\\beta_1 x_t) + \\sigma_\\epsilon^2$$\n",
    "\n",
    "여기서 $E[\\beta_0 + \\beta_1 x_t - \\hat \\beta_0 - \\hat \\beta_1 x_t])^2$는 편향이라고 하고, $\\mathrm{Var}(\\hat\\beta_0 + \\hat\\beta_1 x_t)$는 분산이라고 한다. $\\sigma_\\epsilon^2$은 줄일 수 없는 순수한 데이터의 잡음이다\n",
    "\n",
    "즉, MSE optimization 과정에서 Variance와 Bias의 tradeoff 관계가 존재한다는 것을 유추할 수 있다. 이를 두고 계량경제학과 머신러닝 모형의 목적이 확연히 구분된다\n",
    "\n",
    "- 계량경제학의 목적 : unbiased estimator중에서 Variance를 최소화하는 값을 추정한다 (BLUE, MVUE)\n",
    "- 머신러닝의 목적 : **성능을 최대화하는** 지점에서 Bias와 Variance를 **적절히 조정한다** (minimize MSE)\n",
    "\n",
    "Machine Learning에서는 Bias에 대한 엄격한 가정 **(BLUE, 최소제곱불편추정량)** 을 포기하고, 분산을 최소화하여 예측력을 높이는 것을 목표로 한다. 이 관점에서 기존의 계량경제학과 충돌이 발생한다. Machine Learning에서 추정된 coefficient는 bias를 포함하기 때문에, coefficient는 해석에 있어서 어려움이 있다. 다만, 예측력은 기존 계량경제학 모형에 비해 월등히 좋다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5eb0cbc742c03aa2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Support Vector Regression\n",
    "\n",
    "1995년, Vapnik와 Cortes는 Support Vector Network라는 모형을 제안하였다\n",
    "\n",
    "https://link.springer.com/article/10.1007/BF00994018\n",
    "\n",
    "**2.1 Hard Margin Model**\n",
    "\n",
    "Hard Margin Model의 손실함수는 다음을 최적화하는 것을 목적으로 한다\n",
    "\n",
    "$$\\mathrm{Loss_{SVR}} = \\min_w ||w||^2 + \\lambda(\\frac{1}{2}\\sum_{i = 1}^n (y_i - f(x_i))^2)$$\n",
    "\n",
    "여기서 $||w||^2$는 데이터와 회귀선과의 거리를 나타내는데, 이는 선형회귀식에서 오차를 최소화하는 식과 유사함을 알 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2acdb9cfbf98792d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.2 Soft Margin Model**\n",
    "\n",
    "<center><img src = \"https://leejiyoon52.github.io/images/image_67.png\" alt=\"My Image\"></center>\n",
    "\n",
    "과적합이나 기타 일반화 성능 향상을 위해, 보통은 아래와 같은 손실함수를 최적화하는 것을 목적으로 한다\n",
    "\n",
    "$$\\mathrm{Loss_{SVR}} = \\min_w \\frac{1}{2}||w||^2 + C\\sum_{i = 1}^n (\\xi_i + \\xi_i^*)$$\n",
    "$$\\mathrm{such~that~} (w^T x_i + b) - y_i \\leq \\epsilon + \\xi_i$$\n",
    "$$y_i - (w^T x_i + b) \\leq \\epsilon + \\xi_i^*$$\n",
    "$$\\xi_i, \\xi_i^* \\geq 0$$\n",
    "\n",
    "추정된 회귀식에서 상단과 하단에 각각 $\\epsilon$만큼 마진을 생성하여 추정식의 허용 범위를 넓혀 준다. 즉, margin 값을 크게 줄 수록 모형의 노이즈 허용 범위가 커지는 것이다\n",
    "\n",
    "비용함수 최적화에 있어서 Lagrangian Problem으로 해를 찾는 것이 가능하지만, 그 과정이 길기 때문에 우선은 생략한다. Lagrangian Dual Problem은 다음과 같이 정리된다\n",
    "\n",
    "$$\\mathcal{L_D} = \\frac{1}{2}\\sum_{i,j = 1}^n (\\alpha_i^* - \\alpha_i)(\\alpha_j^* - \\alpha_j)\\mathbf{x_i^Tx_j} - \\epsilon \\sum_{i,j = 1}^n(\\alpha_i^* - \\alpha_i) + \\sum_{i,j = 1}^n y_i(\\alpha_i^* - \\alpha_i)$$\n",
    "$$\\mathrm{such ~ that ~} \\sum_{i=1}^n (\\alpha_i^* - \\alpha_i) = 0, ~~~~~ \\alpha_i, \\alpha_i^* \\in [0,C]$$\n",
    "\n",
    "Lagrangian dual problem으로 재구성한 목적식은 $α$로 이루어져있는 convex하고, 연속적인 quadratic programming problem이다. 최적화 프로그램을 이용한다면 간편하게 $α$를 도출할 수 있다"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c57554cee2c12f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5b28ec334c27ec56"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.3 Kernel Function**\n",
    "\n",
    "Support Vector Machine은 선형 모형이기 때문에 비선형 회귀식을 구해야 하는 경우가 상당히 제약적이었다. 이런 경우 Mapping function의 일종인 Kernel Function을 사용하여 해결하게 되었다\n",
    "\n",
    "$$x = (x_1, x_2, \\dots, x_p) \\Rightarrow \\phi(x) = z = (z_1, z_2, \\dots, z_q)$$\n",
    "\n",
    "<center><img src = \"https://leejiyoon52.github.io/images/image_80.png\" alt=\"My Image\"></center>\n",
    "\n",
    "즉, 중간에 비선형 공간을 추가하여 선형공간으로 다시 데이터를 평평하게 펴는 과정을 거치는 것이라 이해하면 좋다. Kernel Function이 포함된 Lagrangian Dual Problem은 다음과 같이 표현된다\n",
    "\n",
    "$$\\mathcal{L_D} = \\frac{1}{2}\\sum_{i,j = 1}^n (\\alpha_i^* - \\alpha_i)(\\alpha_j^* - \\alpha_j)\\mathbf{K(x_i^,x_j)} - \\epsilon \\sum_{i,j = 1}^n(\\alpha_i^* - \\alpha_i) + \\sum_{i,j = 1}^n y_i(\\alpha_i^* - \\alpha_i)$$\n",
    "\n",
    "평면식과는 다르게 중간 과정에서 Kernel Function의 Mapping과정이 포함된것을 알 수 있다"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c79c90c7604bfd42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "73c405ebc4481aea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Regression Tree\n",
    "\n",
    "Regression Tree는 오차제곱합을 가장 잘 줄일 수 있는 Feature를 기준으로 분기를 만들어 결과를 예측하는 매우 단순한 모형이다. 분기 과정에서 정보가 저장되는데, 이를 이용해 모형 추정 후 어떤 Feature가 예측에 크게 작용했는지 볼 수 있기 때문에 설명력에 있어서 다른 Machine Learning 모형보다 우월하다\n",
    "\n",
    "최근에는 이를 이용하여 Inference를 시도하는 것이 연구되고 있다. 다만 여기서 유의할 점은, Granger Causality와 같이 예측에 있어서 특정 Feature가 큰 영향을 끼쳤다는 것을 보여줄 뿐, **인과관계를 보장하지는 않는다.** 즉, 여전히 인과관계 추론에 있어서는 **Domain Knowledge**가 매우 중요하다\n",
    "\n",
    "아래는 Tree Model이 분기를 하는 과정을 보여준다\n",
    "\n",
    "<center><img src = \"https://www.statology.org/wp-content/uploads/2020/11/tree6.png\" alt=\"My Image\"></center>\n",
    "\n",
    "Model자체가 분기를 포함하기 때문에, 추정된 Regression Line은 각이 진 상태로 나온다. 아래는 Regression Tree가 추정한 Line을 보여준다\n",
    "\n",
    "<center><img src = \"https://cdn-images-1.medium.com/max/1200/1*VD-ygoJ8OwUpBaTgkDTSOg.png\" alt=\"My Image\"></center>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac07597d5115f39b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3.1 Training**\n",
    "\n",
    "Regression Tree는 Top-down 방식으로 진행되며, 탐욕적 알고리즘 탐색 방식으로 가장 좋은 가지를 찾아서 분기를 하는 방식을 반복한다. 모든 변수 $X_1, X_2, \\dots, X_p$와 모든 가능한 분기지점(cut point) $s$에 대해 오차 제곱합을 가장 많이 줄여주는 $(j,s)$를 선택하게 된다. Tree가 두 개의 분기로 나뉘는 초평면은 다음과 같이 정의 가능하다\n",
    "\n",
    "$$R_1(j,s) = \\{X | X_j < s\\} \\mathrm{~~and~~} R_2 (j,s) = \\{X|X_j \\geq s\\}$$\n",
    "\n",
    "다음 수식을 최소화시키는 $j$와 $s$를 구하는 식으로 분기가 이뤄진다\n",
    "\n",
    "$$\\Sigma_{i:x_i \\in R_1(j,s)}(y_i - \\hat{y_{R_1}})^2 + \\Sigma_{i:x_i \\in R_2(j,s)}(y_i - \\hat{y_{R_2}})^2$$\n",
    "\n",
    "여기서 목적이 **최소제곱**이라는 것에 주목하자. $R_1, R_2$로 구획을 나눌 때 $R_1(j,s)$에 속한 데이터들과 구획 내의 평균$\\hat{y_{R_1}}$와의 차이를 구한 뒤 제곱하고, 이를 $R_2$도 마찬가지로 진행한 뒤 모두 더함으로써 오차 제곱합을 구한다\n",
    "\n",
    "분기를 한 뒤 오차 제곱합이 분기 전 오차 제곱합보다 작아질 때까지 위 과정을 반복한다"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1e4dc87f9ef3850"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3.2 Pruning**\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd7603601f99ed39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1cd86392e8492194"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
